{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4dcf21",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-ts\n",
      "  Using cached stable_ts-2.13.7-py3-none-any.whl\n",
      "Requirement already satisfied: torch in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.19.0 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (4.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (1.23.5)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.1.2-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "Requirement already satisfied: ffmpeg-python==0.2.0 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (0.2.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (10.1.0)\n",
      "Requirement already satisfied: openai-whisper==20231117 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (20231117)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from stable-ts) (4.64.1)\n",
      "Requirement already satisfied: future in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from ffmpeg-python==0.2.0->stable-ts) (0.18.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from openai-whisper==20231117->stable-ts) (0.5.2)\n",
      "Requirement already satisfied: numba in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from openai-whisper==20231117->stable-ts) (0.56.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (22.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (0.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->stable-ts) (6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from tqdm->stable-ts) (0.4.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from torch->stable-ts) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from torch->stable-ts) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from torch->stable-ts) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from torch->stable-ts) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from torch->stable-ts) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from jinja2->torch->stable-ts) (2.1.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from numba->openai-whisper==20231117->stable-ts) (0.39.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from numba->openai-whisper==20231117->stable-ts) (65.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->stable-ts) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->stable-ts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->stable-ts) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->stable-ts) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sunil\\anaconda3\\lib\\site-packages (from sympy->torch->stable-ts) (1.2.1)\n",
      "Installing collected packages: torchaudio, stable-ts\n",
      "Successfully installed stable-ts-2.13.7 torchaudio-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stable-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61ee623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import srt\n",
    "import whisper\n",
    "import logging\n",
    "import os\n",
    "from stable_whisper import modify_model\n",
    "logger = logging.getLogger(\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44bb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_openai = \"base.en\"\n",
    "file_path = \"/home/sunil/projects/Stuff/Combined/TalkNet-ASD/Videos/Video4.mp4\"\n",
    "video_dir = \"../Dataset/Videos/\"\n",
    "\n",
    "def get_openai_model(model_name=\"base\"):\n",
    "    # initialize model\n",
    "    logging.info(f\"Initializing openai's '{model_name} 'model\")\n",
    "    if model_name in [\n",
    "        \"tiny.en\",\n",
    "        \"tiny\",\n",
    "        \"base.en\",\n",
    "        \"base\",\n",
    "        \"small.en\",\n",
    "        \"small\",\n",
    "        \"medium.en\",\n",
    "        \"medium\",\n",
    "        \"large\",\n",
    "    ]:\n",
    "        try:\n",
    "            model = whisper.load_model(model_name)\n",
    "            # Using the stable whisper to modifiy the model for better timestamps accuracy\n",
    "            modify_model(model)\n",
    "            logging.info(\"Model was successfully initialized\")\n",
    "        except:\n",
    "            logging.error(\"Unable to initialize openai model\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.error(\n",
    "            \"Model  not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_openai_model(model_name_openai)\n",
    "\n",
    "def generate_openai_transcription(file_name, file_path, model):\n",
    "    logging.info(f\"Generating transcription for file - {file_path}\")\n",
    "\n",
    "    decode_options = dict(language=\"en\")\n",
    "    transcribe_options = dict(task=\"transcribe\", **decode_options)\n",
    "    output = model.transcribe(file_path, **transcribe_options)\n",
    "\n",
    "    transcriptions = {}\n",
    "\n",
    "    for num, s in enumerate(output.segments):\n",
    "        transcriptions[num] = []\n",
    "        for word in s.words:\n",
    "            transcriptions[num].append(\n",
    "                {\n",
    "                    \"text\": s.text,\n",
    "                    \"segment_start\": s.start,\n",
    "                    \"segment_end\": s.end,\n",
    "                    \"word\": word.word,\n",
    "                    \"word_start\": word.start,\n",
    "                    \"word_end\": word.end,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for key, words in transcriptions.items():\n",
    "        for word in words:\n",
    "            row = {\n",
    "                \"file_name\": file_name,\n",
    "                \"segment_id\": key,\n",
    "                \"segment_text\": word[\"text\"],\n",
    "                \"segment_start\": word[\"segment_start\"],\n",
    "                \"segment_end\": word[\"segment_end\"],\n",
    "                \"word\": word[\"word\"],\n",
    "                \"word_start\": word[\"word_start\"],\n",
    "                \"word_end\": word[\"word_end\"],\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebcddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MagnusCarlson_542_599.mp4',\n",
       " 'NDT_India_19_88.mp4',\n",
       " 'StarTalk_CMBR_190_225.mp4',\n",
       " 'StarTalk_CMBR_270_308.mp4',\n",
       " 'StarTalk_CMBR_319_356.mp4',\n",
       " 'StarTalk_CMBR_92_152.mp4',\n",
       " 'StarTalk_FlyingVehicles_1001_1043.mp4',\n",
       " 'StarTalk_FlyingVehicles_1980_2040.mp4',\n",
       " 'StarTalk_FlyingVehicles_2446_2508.mp4',\n",
       " 'StarTalk_FlyingVehicles_2670_2710.mp4',\n",
       " 'StarTalk_FlyingVehicles_300_340.mp4',\n",
       " 'StarTalk_FlyingVehicles_674_719.mp4',\n",
       " 'StarTalk_FlyingVehicles_780_811.mp4',\n",
       " 'StarTalk_FlyingVehicles_949_1000.mp4',\n",
       " 'StarTalk_Sleep_1152_1211.mp4',\n",
       " 'StarTalk_Sleep_1602_1639.mp4',\n",
       " 'StarTalk_Sleep_1980_2041.mp4',\n",
       " 'StarTalk_Sleep_2099_2160.mp4',\n",
       " 'StarTalk_Sleep_2379_2443.mp4',\n",
       " 'StarTalk_Sleep_2470_2551.mp4',\n",
       " 'StarTalk_Sleep_382_450.mp4',\n",
       " 'StarTalk_Sleep_748_796.mp4']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_files = [\n",
    "    f\n",
    "    for f in os.listdir(video_dir)\n",
    "    if os.path.isfile(os.path.join(video_dir, f)) and os.path.splitext(os.path.join(video_dir, f))[1] in [\".mp4\"]\n",
    "]\n",
    "video_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fcb81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57.01/57.01 [00:09<00:00,  5.94sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69.0/69.0 [00:11<00:00,  6.08sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35.0/35.0 [00:05<00:00,  6.16sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38.01/38.01 [00:07<00:00,  5.13sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37.01/37.01 [00:05<00:00,  6.61sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60.02/60.02 [00:10<00:00,  5.89sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42.02/42.02 [00:06<00:00,  6.18sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60.01/60.01 [00:10<00:00,  5.79sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62.01/62.01 [00:10<00:00,  5.91sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40.0/40.0 [00:07<00:00,  5.04sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40.01/40.01 [00:05<00:00,  6.75sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45.01/45.01 [00:07<00:00,  5.74sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31.0/31.0 [00:06<00:00,  4.45sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51.01/51.01 [00:09<00:00,  5.39sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59.01/59.01 [00:07<00:00,  7.84sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37.0/37.0 [00:05<00:00,  6.33sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61.01/61.01 [00:08<00:00,  7.27sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61.01/61.01 [00:09<00:00,  6.26sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64.02/64.02 [00:10<00:00,  6.25sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 81.01/81.01 [00:12<00:00,  6.37sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68.0/68.0 [00:10<00:00,  6.72sec/s]\n",
      "C:\\Users\\Sunil\\anaconda3\\lib\\site-packages\\stable_whisper\\whisper_word_level.py:224: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribe: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:07<00:00,  6.26sec/s]\n"
     ]
    }
   ],
   "source": [
    "# video_files = ['MagnusCarlson_542_599.mp4', 'NDT_India_19_88.mp4']\n",
    "df_list = []\n",
    "for f in video_files:\n",
    "    df_list.append(generate_openai_transcription(f, os.path.join(video_dir, f), model))\n",
    "\n",
    "final_df = pd.concat(df_list)\n",
    "final_df.to_excel(\"Transcriptions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2cbdf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Video4.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
