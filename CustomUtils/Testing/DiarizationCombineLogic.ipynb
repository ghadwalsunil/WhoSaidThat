{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834924f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation, Segment\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e738face",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m load_dotenv()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyannote\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m     10\u001b[0m pretrained_pipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyannote/speaker-diarization-3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                                use_auth_token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHUGGINGFACE_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/__init__.py:1187\u001b[0m\n\u001b[1;32m   1180\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _StorageBase\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/functional.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_einsum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopt_einsum\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lowrank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svd_lowrank, pca_lowrank\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[1;32m     12\u001b[0m     handle_torch_function)\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[1;32m      4\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/nn/modules/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, \\\n\u001b[1;32m      4\u001b[0m     ConvTranspose1d, ConvTranspose2d, ConvTranspose3d, \\\n\u001b[1;32m      5\u001b[0m     LazyConv1d, LazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d, LazyConvTranspose3d\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Threshold, ReLU, Hardtanh, ReLU6, Sigmoid, Tanh, \\\n\u001b[1;32m      7\u001b[0m     Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GELU, Hardshrink, LeakyReLU, LogSigmoid, \\\n\u001b[1;32m      8\u001b[0m     Softplus, Softshrink, MultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU, GLU, \\\n\u001b[1;32m      9\u001b[0m     Hardsigmoid, Hardswish, SiLU, Mish\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter, UninitializedParameter\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/nn/functional.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# The JIT doesn't understand Union, nor torch.dtype here\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     DType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_jit_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[1;32m     22\u001b[0m     handle_torch_function)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _reduction \u001b[38;5;28;01mas\u001b[39;00m _Reduction\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/_jit_internal.py:40\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# This is needed. `torch._jit_internal` is imported before `torch.distributed.__init__`.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Explicitly ask to import `torch.distributed.__init__` first.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Otherwise, \"AttributeError: module 'torch' has no attribute 'distributed'\" is raised.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mangling\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpackage_mangling\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_awaits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Await\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/distributed/__init__.py:58\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distributed_c10d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     54\u001b[0m         HashStore,\n\u001b[1;32m     55\u001b[0m         _round_robin_process_groups,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_c10d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Variables prefixed with underscore are not auto imported\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# See the comment in `distributed_c10d.py` above `_backend` on why we expose\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# this.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_c10d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m     _backend,\n\u001b[1;32m     66\u001b[0m     _all_gather_base,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     _c10d_error_logger,\n\u001b[1;32m     71\u001b[0m )\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:35\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distributed_c10d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     AllreduceCoalescedOptions,\n\u001b[1;32m     18\u001b[0m     AllreduceOptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     Work\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m record_function\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_pg_timeout\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mc10d_error_logger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_or_create_logger\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_ad\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _vmap_internals\n\u001b[1;32m     29\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_mode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/autograd/graph.py:131\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01msaved_tensors_hooks\u001b[39;00m():\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context-manager that sets a pair of pack / unpack hooks for saved tensors.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    Use this context-manager to define how intermediary results of an operation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m        context-manager, only the inner-most pair of hooks will be applied.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pack_hook: Callable[[torch\u001b[38;5;241m.\u001b[39mTensor], Any], unpack_hook: Callable[[Any], torch\u001b[38;5;241m.\u001b[39mTensor]):\n",
      "File \u001b[0;32m~/projects/Stuff/Combined/venv/lib/python3.8/site-packages/torch/autograd/graph.py:192\u001b[0m, in \u001b[0;36msaved_tensors_hooks\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01msaved_tensors_hooks\u001b[39;00m():\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context-manager that sets a pair of pack / unpack hooks for saved tensors.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    Use this context-manager to define how intermediary results of an operation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m        context-manager, only the inner-most pair of hooks will be applied.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pack_hook: \u001b[43mCallable\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m, unpack_hook: Callable[[Any], torch\u001b[38;5;241m.\u001b[39mTensor]):\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_hook \u001b[38;5;241m=\u001b[39m pack_hook\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack_hook \u001b[38;5;241m=\u001b[39m unpack_hook\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:813\u001b[0m, in \u001b[0;36m_VariadicGenericAlias.__getitem__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCallable[args, result]: args must be a list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    812\u001b[0m     params \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mtuple\u001b[39m(args), result)\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitem_inner__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:258\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, subprocess, sys, time\n",
    "from pyannote.core import Segment, Annotation\n",
    "from pyannote.core.notebook import Notebook\n",
    "import matplotlib.pyplot as plt\n",
    "load_dotenv()\n",
    "import pickle, json\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.0\",\n",
    "                                               use_auth_token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "pretrained_pipeline.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_rttm_file = '../../../AVA-AVD/dataset/rttms/2qQs3Y9OJX0_c_01.rttm'\n",
    "offset=900.488\n",
    "encoding_df = pd.read_pickle(\"../../output/video_temp/2qQs3Y9OJX0_c_01/pywork/new_encoding_df.pckl\")\n",
    "audio_path = \"../../output/video_temp/2qQs3Y9OJX0_c_01/pywav/audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "metric = DiarizationErrorRate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rttm_to_diarization(rttm_file, offset):\n",
    "    \n",
    "    # Read RTTM file into pandas DataFrame\n",
    "    rttm_df = pd.read_csv(rttm_file, sep=' ', header=None,\n",
    "                      names=['temp', 'file_name', 'channel', 'start', 'duration', 'NA_1', 'NA_2', 'speaker_label', 'NA_3', 'NA_4'])\n",
    "    \n",
    "    rttm_df.sort_values(by=\"start\", inplace=True)\n",
    "\n",
    "    diarize_dict = {}\n",
    "\n",
    "    # Iterate over RTTM rows and add segments to Pyannote annotation\n",
    "    for _, row in rttm_df.iterrows():\n",
    "        start_time = round(row['start'] - offset, 2)\n",
    "        end_time = round(start_time + row['duration'], 2)\n",
    "        label = row['speaker_label']\n",
    "        if label not in diarize_dict.keys():\n",
    "            diarize_dict[label] = [(start_time, end_time)]\n",
    "        else:\n",
    "            diarize_dict[label].append((start_time, end_time))\n",
    "\n",
    "    return diarize_dict\n",
    "\n",
    "def convert_diarization_output_to_pyannote(diarize_output):\n",
    "    annotation = Annotation()\n",
    "    \n",
    "    for speaker, timelines in diarize_output.items():\n",
    "        for timeline_start, timeline_end in timelines:\n",
    "            annotation[Segment(timeline_start, timeline_end)] = speaker\n",
    "\n",
    "    return annotation\n",
    "\n",
    "def convert_pyannote_to_diarization(pyannote_output):\n",
    "    \n",
    "    diarize_dict = {}\n",
    "    for duration,_, speaker_key in pyannote_output.itertracks(yield_label=True):\n",
    "        start_time = round(duration.start, 2)\n",
    "        end_time = round(duration.end, 2)\n",
    "        if speaker_key in diarize_dict.keys():\n",
    "            diarize_dict[speaker_key].append((start_time,end_time))\n",
    "        else:\n",
    "            diarize_dict[speaker_key] = [(start_time,end_time)]\n",
    "            \n",
    "    return diarize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6694ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_1j20 = convert_rttm_to_diarization(ground_truth_rttm_file, offset)\n",
    "pyannote_gt = convert_diarization_output_to_pyannote(gt_1j20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef32cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f64202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ranges(lst, frame_rate):\n",
    "    ranges = []\n",
    "    start = lst[0]\n",
    "\n",
    "    threshold = round(frame_rate / 10) * 3\n",
    "\n",
    "    for i in range(1, len(lst)):\n",
    "        if lst[i] - lst[i - 1] > threshold:\n",
    "            if lst[i - 1] - start > threshold:\n",
    "                ranges.append(\n",
    "                    (round(start / frame_rate, 2), round(lst[i - 1] / frame_rate, 2))\n",
    "                )\n",
    "            start = lst[i]\n",
    "\n",
    "    # Add the last range\n",
    "    if lst[-1] - start > threshold:\n",
    "        ranges.append((round(start / frame_rate, 2), round(lst[-1] / frame_rate, 2)))\n",
    "\n",
    "    return ranges\n",
    "\n",
    "def get_final_tracks(df, frameRate):\n",
    "    final_tracks = {}\n",
    "    df = df.sort_values(by=[\"Final_Cluster\", \"Frame\"])\n",
    "    for idx in df[\"Final_Cluster\"].unique():\n",
    "        speaker_key = \"SPEAKER_{:02d}\".format(idx)\n",
    "        final_tracks[speaker_key] = df[df[\"Final_Cluster\"] == idx][\"Frame\"].to_list()\n",
    "\n",
    "    for key in final_tracks.keys():\n",
    "        final_tracks[key] = convert_to_ranges(\n",
    "            final_tracks[key], frameRate\n",
    "        )\n",
    "\n",
    "    return final_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0704e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20 = get_final_tracks(encoding_df, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0667dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_vdo = convert_diarization_output_to_pyannote(vdo_1j20)\n",
    "pyannote_vdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric(pyannote_gt, pyannote_vdo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad987fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pyannote_diarization(pretrained_model, audio_path, min_cluster_size=12):\n",
    "    \n",
    "    pretrained_model.instantiate({\n",
    "        \"clustering\" : {\n",
    "            \"min_cluster_size\": min_cluster_size\n",
    "        }\n",
    "    })\n",
    "    pretrained_model.parameters(instantiated=True)\n",
    "    diarization = pretrained_model(audio_path)\n",
    "    \n",
    "    return diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d35776",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_ad03 = perform_pyannote_diarization(pretrained_pipeline, audio_path, min_cluster_size=3)\n",
    "pyannote_ad06 = perform_pyannote_diarization(pretrained_pipeline, audio_path, min_cluster_size=6)\n",
    "pyannote_ad09 = perform_pyannote_diarization(pretrained_pipeline, audio_path, min_cluster_size=9)\n",
    "pyannote_ad12 = perform_pyannote_diarization(pretrained_pipeline, audio_path, min_cluster_size=12)\n",
    "ad03_1j20 = convert_pyannote_to_diarization(pyannote_ad03)\n",
    "ad06_1j20 = convert_pyannote_to_diarization(pyannote_ad06)\n",
    "ad09_1j20 = convert_pyannote_to_diarization(pyannote_ad09)\n",
    "ad12_1j20 = convert_pyannote_to_diarization(pyannote_ad12)\n",
    "pyannote_ad03 = convert_diarization_output_to_pyannote(ad03_1j20)\n",
    "pyannote_ad06 = convert_diarization_output_to_pyannote(ad06_1j20)\n",
    "pyannote_ad09 = convert_diarization_output_to_pyannote(ad09_1j20)\n",
    "pyannote_ad12 = convert_diarization_output_to_pyannote(ad12_1j20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_ad03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad03_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyannoteAudioSegment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        segment_idx,\n",
    "        audio_segment_start,\n",
    "        audio_segment_end,\n",
    "        ad_03_speaker,\n",
    "        ad_06_speaker,\n",
    "        ad_09_speaker,\n",
    "        ad_12_speaker,\n",
    "        has_overlap,\n",
    "        vd_speaker=None,\n",
    "    ):\n",
    "        self.segment_idx = segment_idx\n",
    "        self.audio_segment_start = audio_segment_start\n",
    "        self.audio_segment_end = audio_segment_end\n",
    "        self.vd_speaker = vd_speaker\n",
    "        self.ad_03_speaker = ad_03_speaker\n",
    "        self.ad_06_speaker = ad_06_speaker\n",
    "        self.ad_09_speaker = ad_09_speaker\n",
    "        self.ad_12_speaker = ad_12_speaker\n",
    "        self.has_overlap = has_overlap\n",
    "\n",
    "    def get_speaker(self, group_id):\n",
    "        if group_id == \"ad_03\":\n",
    "            return self.ad_03_speaker\n",
    "        elif group_id == \"ad_06\":\n",
    "            return self.ad_06_speaker\n",
    "        elif group_id == \"ad_09\":\n",
    "            return self.ad_09_speaker\n",
    "        elif group_id == \"ad_12\":\n",
    "            return self.ad_12_speaker\n",
    "        elif group_id == \"vd\":\n",
    "            return self.vd_speaker\n",
    "        else:\n",
    "            print(f\"Invalid group id - {group_id}\")\n",
    "            return None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.audio_segment_start < other.audio_segment_start\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.audio_segment_start > other.audio_segment_start\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.audio_segment_start == other.audio_segment_start\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.audio_segment_start <= other.audio_segment_start\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return self.audio_segment_start >= other.audio_segment_start\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return self.audio_segment_start != other.audio_segment_start\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"PyannoteAudioSegment({self.audio_segment_start}, {self.audio_segment_end}, {self.ad_03_speaker}, {self.ad_06_speaker}, {self.ad_09_speaker}, {self.ad_12_speaker}, {self.vd_speaker})\"\n",
    "\n",
    "\n",
    "class MappingClass:\n",
    "    def __init__(self):\n",
    "        self.mapping_dict = {}\n",
    "\n",
    "    def add_mapping(self, speaker_key_id, speaker_value_id, audio_segment_start, audio_segment_end):\n",
    "        if speaker_value_id == \"Unknown\":\n",
    "            return\n",
    "        if speaker_key_id not in self.mapping_dict.keys():\n",
    "            self.mapping_dict[speaker_key_id] = {speaker_value_id: audio_segment_end - audio_segment_start}\n",
    "        else:\n",
    "            if speaker_value_id not in self.mapping_dict[speaker_key_id].keys():\n",
    "                self.mapping_dict[speaker_key_id][speaker_value_id] = audio_segment_end - audio_segment_start\n",
    "            else:\n",
    "                self.mapping_dict[speaker_key_id][speaker_value_id] += audio_segment_end - audio_segment_start\n",
    "\n",
    "    def get_max_mapping(self, speaker_key_id):\n",
    "        if speaker_key_id not in self.mapping_dict.keys():\n",
    "            return None\n",
    "\n",
    "        return max(self.mapping_dict[speaker_key_id], key=self.mapping_dict[speaker_key_id].get)\n",
    "\n",
    "\n",
    "class DiarizationOutput:\n",
    "    def __init__(\n",
    "        self, audio_diarization_03, audio_diarization_06, audio_diarization_09, audio_diarization_12, video_diarization\n",
    "    ):\n",
    "        self.audio_diarization_03 = audio_diarization_03\n",
    "        self.audio_diarization_06 = audio_diarization_06\n",
    "        self.audio_diarization_09 = audio_diarization_09\n",
    "        self.audio_diarization_12 = audio_diarization_12\n",
    "        self.video_diarization = video_diarization\n",
    "        self.audio_segments: list[PyannoteAudioSegment] = self.get_audio_segments()\n",
    "        self.ad03_video_mapping: MappingClass = self.perform_audio_video_mapping(\"ad_03\")\n",
    "        self.ad06_video_mapping: MappingClass = self.perform_audio_video_mapping(\"ad_06\")\n",
    "        self.ad09_video_mapping: MappingClass = self.perform_audio_video_mapping(\"ad_09\")\n",
    "        self.ad12_video_mapping: MappingClass = self.perform_audio_video_mapping(\"ad_12\")\n",
    "        self.predict_unknowns()\n",
    "\n",
    "    def get_corresponding_speaker(self, present_group_id, target_group_id, speaker_id):\n",
    "\n",
    "        if present_group_id == \"ad_03\":\n",
    "            groups_not_allowed = [\"ad_03\"]\n",
    "        elif present_group_id == \"ad_06\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\"]\n",
    "        elif present_group_id == \"ad_09\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\", \"ad_09\"]\n",
    "        elif present_group_id == \"ad_12\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\", \"ad_09\", \"ad_12\"]\n",
    "        elif present_group_id == \"vd\":\n",
    "            groups_not_allowed = [\"vd\"]\n",
    "        else:\n",
    "            print(f\"Invalid present group id - {present_group_id}\")\n",
    "            return None\n",
    "\n",
    "        if target_group_id in groups_not_allowed:\n",
    "            print(f\"For present_group {present_group_id}, target_group should not be among {groups_not_allowed}\")\n",
    "            return None\n",
    "\n",
    "        present_group = self.get_group_by_id(present_group_id)\n",
    "        if speaker_id not in present_group.keys():\n",
    "            print(f\"Speaker_id {speaker_id} not in present group {present_group_id} keys - {present_group.keys()}\")\n",
    "            return None\n",
    "\n",
    "        speaker_interval = present_group[speaker_id]\n",
    "        target_group = self.get_group_by_id(target_group_id)\n",
    "\n",
    "        return self.get_mapping(speaker_interval, target_group)\n",
    "\n",
    "    def get_all_child_speakers(self, parent_group_id, child_group_id, parent_speaker_id):\n",
    "\n",
    "        if parent_group_id == \"ad_12\":\n",
    "            groups_not_allowed = [\"ad_12\"]\n",
    "        elif parent_group_id == \"ad_09\":\n",
    "            groups_not_allowed = [\"ad_09\", \"ad_12\"]\n",
    "        elif parent_group_id == \"ad_06\":\n",
    "            groups_not_allowed = [\"ad_06\", \"ad_09\", \"ad_12\"]\n",
    "        elif parent_group_id == \"ad_03\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\", \"ad_09\", \"ad_12\"]\n",
    "        elif parent_group_id == \"vd\":\n",
    "            groups_not_allowed = [\"vd\"]\n",
    "        else:\n",
    "            print(f\"Invalid parent group id - {parent_group_id}\")\n",
    "            return None\n",
    "\n",
    "        if child_group_id in groups_not_allowed:\n",
    "            print(f\"For parent_group {child_group_id}, child_group should not be among {groups_not_allowed}\")\n",
    "            return None\n",
    "\n",
    "        parent_group = self.get_group_by_id(parent_group_id)\n",
    "        if parent_speaker_id not in parent_group.keys():\n",
    "            print(\n",
    "                f\"parent_speaker_id {parent_speaker_id} not in parent_group {parent_group_id} keys - {parent_group.keys()}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        child_group = self.get_group_by_id(child_group_id)\n",
    "\n",
    "        child_speakers = []\n",
    "\n",
    "        for child_speaker_id in child_group.keys():\n",
    "            _parent_speaker_id = self.get_corresponding_speaker(\n",
    "                present_group_id=child_group_id, target_group_id=parent_group_id, speaker_id=child_speaker_id\n",
    "            )\n",
    "\n",
    "            if _parent_speaker_id == parent_speaker_id:\n",
    "                child_speakers.append(child_speaker_id)\n",
    "\n",
    "        return child_speakers\n",
    "\n",
    "    def get_native_speakers_in_parent_group(self, child_group_id, parent_group_id, child_speaker_id):\n",
    "\n",
    "        if child_group_id == \"ad_03\":\n",
    "            groups_not_allowed = [\"ad_03\"]\n",
    "        elif child_group_id == \"ad_06\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\"]\n",
    "        elif child_group_id == \"ad_09\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\", \"ad_09\"]\n",
    "        elif child_group_id == \"ad_12\":\n",
    "            groups_not_allowed = [\"ad_03\", \"ad_06\", \"ad_09\", \"ad_12\"]\n",
    "        elif child_group_id == \"vd\":\n",
    "            groups_not_allowed = [\"vd\"]\n",
    "        else:\n",
    "            print(f\"Invalid child group id - {child_group_id}\")\n",
    "            return None\n",
    "\n",
    "        if parent_group_id in groups_not_allowed:\n",
    "            print(f\"For child_group {child_group_id}, parent_group should not be among {groups_not_allowed}\")\n",
    "            return None\n",
    "\n",
    "        child_group = self.get_group_by_id(child_group_id)\n",
    "        if child_speaker_id not in child_group.keys():\n",
    "            print(\n",
    "                f\"child_speaker_id {child_speaker_id} not in child_group {child_speaker_id} keys - {child_group.keys()}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        parent_group = self.get_group_by_id(parent_group_id)\n",
    "\n",
    "        parent_speaker_id = self.get_corresponding_speaker(\n",
    "            target_group_id=parent_group_id, present_group_id=child_group_id, speaker_id=child_speaker_id\n",
    "        )\n",
    "\n",
    "        return self.get_all_child_speakers(\n",
    "            parent_group_id=parent_group_id, child_group_id=child_group_id, parent_speaker_id=parent_speaker_id\n",
    "        )\n",
    "\n",
    "    def get_audio_segments(self):\n",
    "        audio_segment_list = []\n",
    "        segment_idx = 0\n",
    "        for speaker_id in self.audio_diarization_03.keys():\n",
    "            for speech_segment_start, speech_segment_end in self.audio_diarization_03[speaker_id]:\n",
    "                audio_segment = PyannoteAudioSegment(\n",
    "                    segment_idx=segment_idx,\n",
    "                    audio_segment_start=speech_segment_start,\n",
    "                    audio_segment_end=speech_segment_end,\n",
    "                    ad_03_speaker=speaker_id,\n",
    "                    ad_06_speaker=self.get_corresponding_speaker(\n",
    "                        present_group_id=\"ad_03\", target_group_id=\"ad_06\", speaker_id=speaker_id\n",
    "                    ),\n",
    "                    ad_09_speaker=self.get_corresponding_speaker(\n",
    "                        present_group_id=\"ad_03\", target_group_id=\"ad_09\", speaker_id=speaker_id\n",
    "                    ),\n",
    "                    ad_12_speaker=self.get_corresponding_speaker(\n",
    "                        present_group_id=\"ad_03\", target_group_id=\"ad_12\", speaker_id=speaker_id\n",
    "                    ),\n",
    "                    vd_speaker=self.get_mapping([(speech_segment_start, speech_segment_end)], self.video_diarization),\n",
    "                    has_overlap=False,\n",
    "                )\n",
    "                audio_segment_list.append(audio_segment)\n",
    "\n",
    "                segment_idx += 1\n",
    "\n",
    "        audio_segment_list.sort()\n",
    "\n",
    "        # Check whether each audio segment has overlap with other audio segments\n",
    "        for i in range(len(audio_segment_list)):\n",
    "            for j in range(i + 1, len(audio_segment_list)):\n",
    "                if audio_segment_list[i].audio_segment_end > audio_segment_list[j].audio_segment_start:\n",
    "                    audio_segment_list[i].has_overlap = True\n",
    "                    audio_segment_list[j].has_overlap = True\n",
    "                    audio_segment_list[i].vd_speaker = \"Unknown\"\n",
    "                    audio_segment_list[j].vd_speaker = \"Unknown\"\n",
    "\n",
    "        return audio_segment_list\n",
    "\n",
    "    def perform_audio_video_mapping(self, audio_group_id):\n",
    "        mapping_class = MappingClass()\n",
    "        for audio_segment in self.audio_segments:\n",
    "            if audio_group_id == \"ad_03\":\n",
    "                mapping_class.add_mapping(\n",
    "                    speaker_key_id=audio_segment.ad_03_speaker,\n",
    "                    speaker_value_id=audio_segment.vd_speaker,\n",
    "                    audio_segment_start=audio_segment.audio_segment_start,\n",
    "                    audio_segment_end=audio_segment.audio_segment_end,\n",
    "                )\n",
    "            elif audio_group_id == \"ad_06\":\n",
    "                mapping_class.add_mapping(\n",
    "                    speaker_key_id=audio_segment.ad_06_speaker,\n",
    "                    speaker_value_id=audio_segment.vd_speaker,\n",
    "                    audio_segment_start=audio_segment.audio_segment_start,\n",
    "                    audio_segment_end=audio_segment.audio_segment_end,\n",
    "                )\n",
    "            elif audio_group_id == \"ad_09\":\n",
    "                mapping_class.add_mapping(\n",
    "                    speaker_key_id=audio_segment.ad_09_speaker,\n",
    "                    speaker_value_id=audio_segment.vd_speaker,\n",
    "                    audio_segment_start=audio_segment.audio_segment_start,\n",
    "                    audio_segment_end=audio_segment.audio_segment_end,\n",
    "                )\n",
    "            elif audio_group_id == \"ad_12\":\n",
    "                mapping_class.add_mapping(\n",
    "                    speaker_key_id=audio_segment.ad_12_speaker,\n",
    "                    speaker_value_id=audio_segment.vd_speaker,\n",
    "                    audio_segment_start=audio_segment.audio_segment_start,\n",
    "                    audio_segment_end=audio_segment.audio_segment_end,\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Invalid audio_group_id - {audio_group_id}\")\n",
    "                return None\n",
    "\n",
    "        return mapping_class\n",
    "\n",
    "    def predict_unknowns(self):\n",
    "        for audio_segment in self.audio_segments:\n",
    "            if audio_segment.vd_speaker == \"Unknown\":\n",
    "                vd_speaker = self.ad03_video_mapping.get_max_mapping(audio_segment.ad_03_speaker)\n",
    "                if vd_speaker is not None:\n",
    "                    audio_segment.vd_speaker = vd_speaker\n",
    "                    self.ad03_video_mapping.add_mapping(\n",
    "                        speaker_key_id=audio_segment.ad_03_speaker,\n",
    "                        speaker_value_id=vd_speaker,\n",
    "                        audio_segment_start=audio_segment.audio_segment_start,\n",
    "                        audio_segment_end=audio_segment.audio_segment_end,\n",
    "                    )\n",
    "                else:\n",
    "                    audio_segment.vd_speaker = f\"Unknown_{audio_segment.ad_03_speaker}\"\n",
    "#                     vd_speaker = self.ad06_video_mapping.get_max_mapping(audio_segment.ad_06_speaker)\n",
    "#                     if vd_speaker is not None:\n",
    "#                         audio_segment.vd_speaker = vd_speaker\n",
    "#                         self.ad06_video_mapping.add_mapping(\n",
    "#                             speaker_key_id=audio_segment.ad_06_speaker,\n",
    "#                             speaker_value_id=vd_speaker,\n",
    "#                             audio_segment_start=audio_segment.audio_segment_start,\n",
    "#                             audio_segment_end=audio_segment.audio_segment_end,\n",
    "#                         )\n",
    "#                     else:\n",
    "#                         audio_segment.vd_speaker = f\"Unknown_{audio_segment.ad_06_speaker}\"\n",
    "#                         vd_speaker = self.ad09_video_mapping.get_max_mapping(audio_segment.ad_09_speaker)\n",
    "#                         if vd_speaker is not None:\n",
    "#                             audio_segment.vd_speaker = vd_speaker\n",
    "#                             self.ad09_video_mapping.add_mapping(\n",
    "#                                 speaker_key_id=audio_segment.ad_09_speaker,\n",
    "#                                 speaker_value_id=vd_speaker,\n",
    "#                                 audio_segment_start=audio_segment.audio_segment_start,\n",
    "#                                 audio_segment_end=audio_segment.audio_segment_end,\n",
    "#                             )\n",
    "#                         else:\n",
    "#                             audio_segment.vd_speaker = f\"Unknown_{audio_segment.ad_09_speaker}\"\n",
    "#                             vd_speaker = self.ad12_video_mapping.get_max_mapping(audio_segment.ad_12_speaker)\n",
    "#                             if vd_speaker is not None:\n",
    "#                                 audio_segment.vd_speaker = vd_speaker\n",
    "#                                 self.ad12_video_mapping.add_mapping(\n",
    "#                                     speaker_key_id=audio_segment.ad_12_speaker,\n",
    "#                                     speaker_value_id=vd_speaker,\n",
    "#                                     audio_segment_start=audio_segment.audio_segment_start,\n",
    "#                                     audio_segment_end=audio_segment.audio_segment_end,\n",
    "#                                 )\n",
    "#                             else:\n",
    "#                                 audio_segment.vd_speaker = f\"Unknown_{audio_segment.ad_12_speaker}\"\n",
    "\n",
    "    def get_diarization_output(self):\n",
    "        diarize_output = {}\n",
    "        for audio_segment in self.audio_segments:\n",
    "            if audio_segment.vd_speaker not in diarize_output.keys():\n",
    "                diarize_output[audio_segment.vd_speaker] = []\n",
    "            diarize_output[audio_segment.vd_speaker].append(\n",
    "                (audio_segment.audio_segment_start, audio_segment.audio_segment_end)\n",
    "            )\n",
    "        return diarize_output\n",
    "\n",
    "    def get_group_by_id(self, group_id):\n",
    "        if group_id == \"ad_03\":\n",
    "            return self.audio_diarization_03\n",
    "        if group_id == \"ad_06\":\n",
    "            return self.audio_diarization_06\n",
    "        if group_id == \"ad_09\":\n",
    "            return self.audio_diarization_09\n",
    "        if group_id == \"ad_12\":\n",
    "            return self.audio_diarization_12\n",
    "        if group_id == \"vd\":\n",
    "            return self.video_diarization\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mapping(speaker_interval, target_group):\n",
    "        max_overlap = 0\n",
    "        max_overlap_speaker = \"Unknown\"\n",
    "        for speaker_id in target_group.keys():\n",
    "            result_overlap, _ = find_overlap(\n",
    "                speaker_interval,\n",
    "                target_group[speaker_id],\n",
    "            )\n",
    "            if result_overlap > max_overlap:\n",
    "                max_overlap = result_overlap\n",
    "                max_overlap_speaker = speaker_id\n",
    "\n",
    "        return max_overlap_speaker\n",
    "\n",
    "\n",
    "def find_overlap(intervals1, intervals2):\n",
    "    overlap = 0\n",
    "    total_duration1 = 0\n",
    "    total_duration2 = 0\n",
    "\n",
    "    # Calculate the total duration of intervals in intervals1\n",
    "    for start, end in intervals1:\n",
    "        total_duration1 += end - start\n",
    "\n",
    "    # Calculate the total duration of intervals in intervals2 and find the overlap\n",
    "    for start, end in intervals2:\n",
    "        total_duration2 += end - start\n",
    "        for s1, e1 in intervals1:\n",
    "            common_start = max(s1, start)\n",
    "            common_end = min(e1, end)\n",
    "            if common_start < common_end:\n",
    "                overlap += common_end - common_start\n",
    "\n",
    "    # Calculate the percentage of overlap with respect to intervals1\n",
    "    percentage_overlap1 = (overlap / total_duration1) * 100\n",
    "\n",
    "    # Calculate the percentage of overlap with respect to intervals2\n",
    "    percentage_overlap2 = (overlap / total_duration2) * 100\n",
    "\n",
    "    return percentage_overlap1, percentage_overlap2\n",
    "\n",
    "\n",
    "diarize_output = DiarizationOutput(\n",
    "    audio_diarization_03=ad03_1j20,\n",
    "    audio_diarization_06=ad06_1j20,\n",
    "    audio_diarization_09=ad09_1j20,\n",
    "    audio_diarization_12=ad12_1j20,\n",
    "    video_diarization=vdo_1j20,\n",
    ")\n",
    "\n",
    "\n",
    "final_diarization_output = diarize_output.get_diarization_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_combined = convert_diarization_output_to_pyannote(final_diarization_output)\n",
    "pyannote_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_diarization_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyannote_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric(pyannote_gt, pyannote_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b137af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97580803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ground_truth_rttm_file = '../../../AVA-AVD/dataset/rttms/1j20qq1JyX4_c_01.rttm'\n",
    "temp_diarization = convert_rttm_to_diarization(temp_ground_truth_rttm_file, 900.488)\n",
    "convert_diarization_output_to_pyannote(temp_diarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_output.get_all_child_speakers(\"vd\", \"ad_03\", \"SPEAKER_00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c786ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation_plot(\n",
    "    diarization_output,\n",
    "    save_path,\n",
    "    video_name,\n",
    "    video_duration,\n",
    "    plot_name=\"diarization\",\n",
    "    offset = 0\n",
    "):\n",
    "    custom_diarization = Annotation()\n",
    "\n",
    "    for speaker_key in diarization_output.keys():\n",
    "        for timeline in diarization_output[speaker_key]:\n",
    "            custom_diarization[Segment(timeline[0], timeline[1])] = speaker_key\n",
    "\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "    # Plot the custom diarization result\n",
    "    nb = Notebook()\n",
    "    nb.plot_annotation(custom_diarization, ax, legend=True)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_yticks([])  # To hide the y-axis\n",
    "    ax.set_xlim(offset, video_duration + offset)\n",
    "\n",
    "    # Save the figure\n",
    "    saveFileName = os.path.join(save_path, f\"{video_name}_{plot_name}.png\")\n",
    "    fig.savefig(saveFileName, bbox_inches=\"tight\")\n",
    "    # Close the figure\n",
    "    ax.clear()\n",
    "    plt.close(fig)\n",
    "    \n",
    "# def convert_diarization_output_to_pyannote(diarize_output, video_name, offset):\n",
    "#     annotation = Annotation()\n",
    "    \n",
    "#     _diarization_output = diarize_output[video_name]\n",
    "#     diarize_dict = {}\n",
    "    \n",
    "#     for speaker, timelines in _diarization_output.items():\n",
    "#         for timeline_start, timeline_end in timelines:\n",
    "#             annotation[Segment(timeline_start + offset, timeline_end + offset)] = speaker\n",
    "#             if speaker not in diarize_dict.keys():\n",
    "#                 diarize_dict[speaker] = [(timeline_start + offset, timeline_end + offset)]\n",
    "#             else:\n",
    "#                 diarize_dict[speaker].append((timeline_start + offset, timeline_end + offset))\n",
    "\n",
    "#     return annotation, diarize_dict\n",
    "\n",
    "# def _convert_diarization_output_to_pyannote(diarize_output):\n",
    "#     annotation = Annotation()\n",
    "    \n",
    "#     for speaker, timelines in diarize_output.items():\n",
    "#         for timeline_start, timeline_end in timelines:\n",
    "#             annotation[Segment(timeline_start, timeline_end)] = speaker\n",
    "\n",
    "#     return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd379cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 899.993\n",
    "videoDuration = len(AudioSegment.from_file(audio_path)) / 1000\n",
    "\n",
    "audio_path = \"../../output/video_temp/1j20qq1JyX4_c_01/pywav/audio.wav\"\n",
    "pretrained_pipeline.instantiate({\n",
    "    \"clustering\" : {\n",
    "        \"min_cluster_size\": 3\n",
    "    }\n",
    "})\n",
    "pretrained_pipeline.parameters(instantiated=True)\n",
    "diarization = pretrained_pipeline(audio_path)\n",
    "ado_1j20_03 = {}\n",
    "for duration,_, speaker_key in diarization.itertracks(yield_label=True):\n",
    "    if speaker_key in ado_1j20_03.keys():\n",
    "        ado_1j20_03[speaker_key].append((duration.start + offset,duration.end + offset))\n",
    "    else:\n",
    "        ado_1j20_03[speaker_key] = [(duration.start + offset,duration.end + offset)]\n",
    "        \n",
    "\n",
    "pretrained_pipeline.instantiate({\n",
    "    \"clustering\" : {\n",
    "        \"min_cluster_size\": 6\n",
    "    }\n",
    "})\n",
    "pretrained_pipeline.parameters(instantiated=True)\n",
    "diarization = pretrained_pipeline(audio_path)\n",
    "ado_1j20_06 = {}\n",
    "for duration,_, speaker_key in diarization.itertracks(yield_label=True):\n",
    "    if speaker_key in ado_1j20_06.keys():\n",
    "        ado_1j20_06[speaker_key].append((duration.start + offset,duration.end + offset))\n",
    "    else:\n",
    "        ado_1j20_06[speaker_key] = [(duration.start + offset,duration.end + offset)]\n",
    "        \n",
    "pretrained_pipeline.instantiate({\n",
    "    \"clustering\" : {\n",
    "        \"min_cluster_size\": 9\n",
    "    }\n",
    "})\n",
    "pretrained_pipeline.parameters(instantiated=True)\n",
    "diarization = pretrained_pipeline(audio_path)\n",
    "ado_1j20_09 = {}\n",
    "for duration,_, speaker_key in diarization.itertracks(yield_label=True):\n",
    "    if speaker_key in ado_1j20_09.keys():\n",
    "        ado_1j20_09[speaker_key].append((duration.start + offset,duration.end + offset))\n",
    "    else:\n",
    "        ado_1j20_09[speaker_key] = [(duration.start + offset,duration.end + offset)]\n",
    "\n",
    "        \n",
    "pretrained_pipeline.instantiate({\n",
    "    \"clustering\" : {\n",
    "        \"min_cluster_size\": 12\n",
    "    }\n",
    "})\n",
    "pretrained_pipeline.parameters(instantiated=True)\n",
    "diarization = pretrained_pipeline(audio_path)\n",
    "ado_1j20_12 = {}\n",
    "for duration,_, speaker_key in diarization.itertracks(yield_label=True):\n",
    "    if speaker_key in ado_1j20_12.keys():\n",
    "        ado_1j20_12[speaker_key].append((duration.start + offset,duration.end + offset))\n",
    "    else:\n",
    "        ado_1j20_12[speaker_key] = [(duration.start + offset,duration.end + offset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_pipeline.instantiate({\n",
    "    \"clustering\" : {\n",
    "        \"min_cluster_size\": 1\n",
    "    }\n",
    "})\n",
    "pretrained_pipeline.parameters(instantiated=True)\n",
    "diarization = pretrained_pipeline(audio_path)\n",
    "ado_1j20_02 = {}\n",
    "for duration,_, speaker_key in diarization.itertracks(yield_label=True):\n",
    "    if speaker_key in ado_1j20_02.keys():\n",
    "        ado_1j20_02[speaker_key].append((duration.start + offset,duration.end + offset))\n",
    "    else:\n",
    "        ado_1j20_02[speaker_key] = [(duration.start + offset,duration.end + offset)]\n",
    "create_annotation_plot(ado_1j20_02, \".\", \"1j20_02\", videoDuration, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_annotation_plot(ado_1j20_03, \".\", \"1j20_03\", videoDuration, offset=offset)\n",
    "create_annotation_plot(ado_1j20_06, \".\", \"1j20_06\", videoDuration, offset=offset)\n",
    "create_annotation_plot(ado_1j20_09, \".\", \"1j20_09\", videoDuration, offset=offset)\n",
    "create_annotation_plot(ado_1j20_12, \".\", \"1j20_12\", videoDuration, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f60c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo = pickle.load(open(\"../../output/run_output/video_diarization_output_AVA_AVD.pckl\", \"rb\"))\n",
    "pyannote_vdo, vdo_1j20 = convert_diarization_output_to_pyannote(vdo, \"1j20qq1JyX4_c_01\", offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad260ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_annotation_plot(vdo_1j20, \".\", \"vdo_1j20\", videoDuration, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d097e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = pretrained_pipeline(audio_path)\n",
    "ado_1j20_06 = {}\n",
    "for duration,_, speaker_key in diarization.itertracks(yield_label=True):\n",
    "    if speaker_key in ado_1j20_06.keys():\n",
    "        ado_1j20_06[speaker_key].append((duration.start,duration.end))\n",
    "    else:\n",
    "        ado_1j20_06[speaker_key] = [(duration.start,duration.end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57526362",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d03b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ado_1j20_09.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8232e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ce2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a96964",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020e3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bebe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcd0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91081de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110f194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyanote_03 = _convert_diarization_output_to_pyannote(ado_1j20_03)\n",
    "pyanote_06 = _convert_diarization_output_to_pyannote(ado_1j20_06)\n",
    "pyanote_09 = _convert_diarization_output_to_pyannote(ado_1j20_09)\n",
    "pyanote_12 = _convert_diarization_output_to_pyannote(ado_1j20_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e5fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20_annotation = convert_diarization_output_to_pyannote(vdo_1j20, 899.993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rttm_to_pyannote(rttm_file, offset):\n",
    "    # Read RTTM file into pandas DataFrame\n",
    "    rttm_df = pd.read_csv(rttm_file, sep=' ', header=None,\n",
    "                          names=['temp', 'file_name', 'channel', 'start', 'duration', 'NA_1', 'NA_2', 'speaker_label', 'NA_3', 'NA_4'])\n",
    "\n",
    "    # Initialize Pyannote annotation\n",
    "    annotation = Annotation()\n",
    "    diarize_dict = {}\n",
    "\n",
    "    # Iterate over RTTM rows and add segments to Pyannote annotation\n",
    "    for _, row in rttm_df.iterrows():\n",
    "        start_time = row['start']\n",
    "        end_time = start_time + row['duration']\n",
    "        label = row['speaker_label']\n",
    "        annotation[Segment(start_time, end_time)] = label\n",
    "        if label not in diarize_dict.keys():\n",
    "            diarize_dict[label] = [(start_time, end_time)]\n",
    "        else:\n",
    "            diarize_dict[label].append((start_time, end_time))\n",
    "\n",
    "    return annotation, diarize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "rttm_file = '../../../AVA-AVD/dataset/rttms/1j20qq1JyX4_c_01.rttm'\n",
    "ground_truth, diarize_dict = convert_rttm_to_pyannote(rttm_file, offset=899.993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49065c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_annotation_plot(diarize_dict, \".\", \"1j20_gt\", videoDuration, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1j20_pyannote_vdo.rttm','w') as f:\n",
    "    pyannote_vdo.write_rttm(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8160015",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e776d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "rttm_file = '../../../AVA-AVD/save/token/avaavd/rttms/1j20qq1JyX4_c_01.rttm'\n",
    "predicted, predicted_dict = convert_rttm_to_pyannote(rttm_file, offset=899.993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e933c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_annotation_plot(predicted_dict, \".\", \"1j20_predicted\", videoDuration, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "metric = DiarizationErrorRate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca638121",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric(ground_truth, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric(ground_truth, pyanote_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ado_1j20_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1750e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(diarization_result_1, diarization_result_2):\n",
    "    result_mapping = {}\n",
    "    unknown_speaker_count = 0\n",
    "    for speaker_result_2 in diarization_result_2.keys():\n",
    "        max_overlap = 0\n",
    "        max_overlap_speaker = None\n",
    "        for speaker_result_1 in diarization_result_1.keys():\n",
    "            result_overlap, _ = find_overlap(\n",
    "                diarization_result_1[speaker_result_1],\n",
    "                diarization_result_2[speaker_result_2],\n",
    "            )\n",
    "            if result_overlap > max_overlap:\n",
    "                max_overlap = result_overlap\n",
    "                max_overlap_speaker = speaker_result_1\n",
    "\n",
    "        if max_overlap_speaker is None or max_overlap < 10:\n",
    "            result_mapping[speaker_result_2] = f\"Unknown_{unknown_speaker_count}\"\n",
    "            unknown_speaker_count += 1\n",
    "        else:\n",
    "            result_mapping[speaker_result_2] = max_overlap_speaker\n",
    "\n",
    "    return result_mapping\n",
    "\n",
    "def get_audio_video_mapping(final_video_output, final_audio_output):\n",
    "\n",
    "    final_audio_video_mapping = get_mapping(\n",
    "            final_video_output, final_audio_output\n",
    "        )\n",
    "    \n",
    "    final_video_audio_mapping = get_mapping(\n",
    "        final_audio_output, final_video_output\n",
    "    )\n",
    "\n",
    "    return final_audio_video_mapping, final_video_audio_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_vi_mapping, vi_au_mapping = get_audio_video_mapping(ado_1j20_03, vdo_1j20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_vi_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beeb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_au_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c7b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_1j20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_output = DiarizationOutput(\n",
    "    audio_diarization_03=ado_1j20_03,\n",
    "    audio_diarization_06=ado_1j20_06,\n",
    "    audio_diarization_09=ado_1j20_09,\n",
    "    audio_diarization_12=ado_1j20_12,\n",
    "    video_diarization=vdo_1j20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e913a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_output.get_corresponding_speaker(present_group_id=\"ad_09\", target_group_id=\"ad_12\", speaker_id=\"SPEAKER_04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01308711",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_output.get_all_child_speakers(parent_group_id=\"ad_12\", child_group_id=\"ad_09\", parent_speaker_id=\"SPEAKER_03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_output.get_native_speakers_in_parent_group(parent_group_id=\"ad_12\", child_group_id=\"ad_09\", child_speaker_id=\"SPEAKER_06\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
